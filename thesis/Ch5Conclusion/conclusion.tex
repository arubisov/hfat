\chapter{Conclusion}

\section{Summary and Future Work}
The backtesting results indicate that implementing the stochastic strategies on just two stocks would have generated a 652\% return on investment in 2014. In addition, however, recurring fees would have included the co-location fees of storage rental and a cross-link to the exchange, approximately \$15,000 per month, and a subscription to the ITCH feed as well as access to NASDAQ's order submission protocol, another \$15,000 per month. Considering that there are other highly liquid shares with small bid-ask spreads listed on the NASDAQ such as \texttt{DELL} or \texttt{MSFT}, of which some may produce similar backtesting results, all signs point to the stochastic strategies being effective and worth taking to market.

Of course, this is a major oversimplification of reality. There are a number of assumptions and modelling choices that were made over the course of this research that must first be revisited prior to attempting forward performance testing (paper trading), let alone taking the system live. In what follows, I address what I consider to be the critical items among the list.

\paragraph{Market order costs.} Perhaps the easiest change of all, the dynamic programming equations need to account for the fact that market order executions come with a cost from the exchange, as they are essentially taking away liquidity. (Posting, modifying, and cancelling limit orders are all free transactions.) Specifically, the market order cost $c$ would have to appear in the stopping regions/impulse controls of the DPEs. The presumed effect would be widening the upper bound on $\delta^\pm$ from $1/\kappa$ to $1/\kappa + c$. This would also presumably decrease the overall incidence of market order executions by the strategies.

\paragraph{Discrete posting depths in increments of 1 tick.} Presently, the posting depths $\delta^\pm$ are continuous variables, and for example in \autoref{fig:comp_dp_z8} were seen to be between 0 and \$0.01. In reality, one can only post orders in depth increments of 1 tick, which for the majority of stocks is equal to \$0.01. Thus, the current results would have us post at illegal depths. This can partially be solved by rounding the results to the nearest cent, but those rounded depths would no longer have the support of the mathematical derivations, and would likely result in a marked decrease in revenue.

\paragraph{Short term price impact.} In order to realize the \$2 million profit, we would be executing orders in sizes of 100 shares at a time. In the case of \texttt{AAPL} this would have us trading 600k-700k shares per day. \texttt{AAPL} has an average daily trading volume of 50m shares, thus we would be contributing more than 1\% of daily volume. It is very likely that due to the large quantity of shares we would be transacting each day, our trading would have an impact on the stock price, which generally has an adverse effect on PnL. Modelling this price impact would be an essential component of either generating more realistic returns, or reducing our daily number of trades.

\paragraph{Accounting for non-homogeneity.} Currently the calibration method uses entire trading days to determine the value function $H$ and the posting depths $\delta^\pm$, which themselves are applied over the whole trading day. However, we saw in the cross-validation section that there are strong grounds for rejecting the time-homogeneity assumption in the underlying data. A feasible extension would be to have independent calibrations for the first hour of trading, last hour of trading, and mid-day trading, as the three are known to have very distinct behaviours. 

\paragraph{Backtesting engine: tracking LOB queue position.} The backtesting engine that has been used to compute the results uses the $e^{-\kappa \delta}$ limit order fill probability assumption that was made in the stochastic optimal control chapter. Largely, this is because we do not presently track our position in the limit order book queue, and this provides a workaround with some empirical credence. Thus, at present, random numbers effectively determine whether our limit orders get filled, and in particular, a depth of $\delta=0$ implies guaranteed execution. However, as we have the ability to reconstruct the entire limit order book from the ITCH data, we thus have all the data we need to actually track our position in the queue, and know with certain whether our order would have been partially or fully executed. Additionally, it's currently implicit that at every timestep we cancel our existing order and repost, even if at the same depth; queue-tracking would force us to make this explicit, adding the option of modifying or keeping our existing orders from the previous timestep. 

\paragraph{Backtesting engine: latency.} The backtesting engine allows for immediate execution of market orders and posting of limit orders, which ignores the time that a signal takes to be sent from the trading system to the exchange server. As was mentioned in the introduction, minimizing latency is a critical consideration in high-frequency algorithmic trading, and is the justification for the large co-location expenses. Thus it is also paramount to account for its existence in the backtesting engine. A simple 2-5ms lag in execution would realistically simulate the time taken to learn of an event, generate a response, and have the exchange act on the response \citep{hasbrouck2013low}.

\paragraph{Early cut-off with optimal liquidation.} As was noted in the stochastic optimal control sections, it is never optimal to wait until maturity and pay the liquidation penalty $\phi$ per share - thus market orders were executed in bulk immediately prior with no penalty. An alternative would be to determine an early cut-off time, on the order of minutes, at which the wealth maximization strategy is ended, and proceed to the end of the day with an optimal liquidation/acquisition strategy. 